ğŸ“‹ Project Description
This project implements a complete, production-ready machine learning pipeline for predicting customer churn using the Telco Customer Churn dataset. The solution demonstrates best practices for building reusable ML pipelines with scikit-learn's Pipeline API, including data preprocessing, feature engineering, model training, hyperparameter tuning, and model deployment.
The pipeline handles the entire ML workflow from raw data to predictions, making it easy to deploy, maintain, and update. The project emphasizes code reproducibility, modular design, and production readiness.

ğŸš€ Features
â€¢	Complete Data Preprocessing: Automated handling of missing values, categorical encoding, and feature scaling
â€¢	Modular Pipeline Design: Separated preprocessing and modeling steps for maintainability
â€¢	Multiple Algorithm Support: Implementation of both Logistic Regression and Random Forest classifiers
â€¢	Hyperparameter Optimization: Automated tuning using GridSearchCV
â€¢	Model Evaluation: Comprehensive metrics including accuracy, precision, recall, F1-score, and ROC curves
â€¢	Production Export: Complete pipeline serialization using joblib for easy deployment
â€¢	Interactive Web Interface: Streamlit app for real-time predictions

ğŸ“Š Dataset
The project uses the Telco Customer Churn dataset, which contains information about telecom customers and whether they churned (left the service).
Dataset Features:
â€¢	Demographic info (gender, age, partner, dependents)
â€¢	Account information (tenure, contract type, payment method)
â€¢	Service details (phone service, internet service, online security, etc.)
â€¢	Charges (monthly charges, total charges)
â€¢	Target variable: Churn (Yes/No)

ğŸ› ï¸ Technical Stack
â€¢	Python 3.8+
â€¢	scikit-learn: Pipeline construction, preprocessing, and modeling
â€¢	pandas & numpy: Data manipulation and analysis
â€¢	matplotlib & seaborn: Data visualization
â€¢	joblib: Model serialization and persistence
â€¢	Streamlit: Web application deployment

ğŸ“ Project Structure
text
End_to_End_ML_Pipeline/
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ telco_churn.csv          # Raw dataset
â”‚
â”œâ”€â”€ models/
â”‚   â””â”€â”€ churn_prediction_pipeline.joblib  # Serialized pipeline
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ Telco_Churn_Analysis.ipynb        # Exploratory data analysis
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_preprocessing.py    # Data cleaning and preparation
â”‚   â”œâ”€â”€ model_training.py        # Pipeline construction and training
â”‚   â””â”€â”€ evaluation.py            # Model evaluation metrics
â”‚
â”œâ”€â”€ app.py                       # Streamlit web application
â”œâ”€â”€ requirements.txt             # Python dependencies
â””â”€â”€ README.md                    # Project documentation
ğŸ¯ Implementation Details
Data Preprocessing
â€¢	Automated handling of missing values in TotalCharges
â€¢	One-hot encoding for categorical variables
â€¢	Standard scaling for numerical features
â€¢	Target variable transformation (Yes/No to 1/0)
ML Pipeline Architecture
python
# Numerical features pipeline
numerical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

# Categorical features pipeline  
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Combined preprocessing
preprocessor = ColumnTransformer(transformers=[
    ('num', numerical_transformer, numerical_cols),
    ('cat', categorical_transformer, categorical_cols)
])

# Complete pipeline
pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', RandomForestClassifier())
])
Model Training & Optimization
â€¢	Implemented Logistic Regression and Random Forest algorithms
â€¢	Hyperparameter tuning using GridSearchCV
â€¢	Cross-validation for robust performance estimation
â€¢	Best model selection based on ROC AUC score
Evaluation Metrics
â€¢	Accuracy: 80.5%
â€¢	Precision: 67.9%
â€¢	Recall: 53.7%
â€¢	F1-score: 60.0%
â€¢	ROC AUC: 0.85

ğŸ“ˆ Results
The optimized Random Forest model achieved:
â€¢	Test Accuracy: 80.5%
â€¢	ROC AUC Score: 0.85
â€¢	Top Features: Contract type, tenure, internet service type, and payment method
The model effectively identifies customers at risk of churning, enabling proactive retention strategies.

ğŸš€ Getting Started
Prerequisites
â€¢	Python 3.8+
â€¢	pip package manager
Installation
1.	Clone the repository:
bash
git clone https://github.com/taimourmushtaq/ChurnPrediction-.git
cd End_to_End_ML_Pipeline
2.	Install dependencies:
bash
pip install -r requirements.txt
3.	Download the dataset from Kaggle and place it in the data/ directory.
Usage
1.	Run the complete pipeline:
bash
python src/model_training.py
2.	Launch the web application:
bash
streamlit run app.py
3.	Access the application at http://localhost:8501

ğŸ”§ Customization
To adapt this pipeline for your own dataset:
1.	Modify data_preprocessing.py to handle your data schema
2.	Update the feature lists in model_training.py
3.	Adjust hyperparameters in the GridSearchCV configuration
4.	Customize the Streamlit app interface in app.py


ğŸ¤ Contributing
1.	Fork the project
2.	Create your feature branch (git checkout -b feature/AmazingFeature)
3.	Commit your changes (git commit -m 'Add some AmazingFeature')
4.	Push to the branch (git push origin feature/AmazingFeature)
5.	Open a Pull Request


